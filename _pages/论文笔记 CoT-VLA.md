---
layout: article
title: CoT-VLA
permalink: /cot-vla/
---





## **📍1. 研究背景（Research Background）**

我们先从**VLA（Vision-Language-Action）模型**说起，这是近年来在机器人操作领域特别火的一类模型。

### **VLA 是什么？**

它的目标是：**给定一段自然语言指令（语言）和当前的观察图像（视觉），输出一系列动作（行动）来完成任务**。

比如：

> 输入：“把红色杯子放在桌子上” + 当前摄像头图像 → 输出：机器人如何移动手臂去完成这个任务。

------



## **🧠2. 当前的挑战 / 提出的问题（Motivation & Problem Statement）**

传统的 VLA 模型一般是「**端到端**」的，直接从输入的图像和语言生成动作。

这种方式的问题是：

1. **缺乏推理能力**：复杂任务（如多步任务）需要中间思考过程，但端到端方法无法显式地规划子目标或中间步骤。
2. **难以泛化**：在真实世界中，任务多样，如果没有清晰的“想法链条”（类似人类的思考过程），模型很容易失败。
3. **无法利用无标签数据**：许多视频数据没有动作标签，无法用于训练这种端到端系统。

### **所以，他们提出了一个问题：**

> **我们能否像人类一样“边想边做”，先视觉化地推理每一步，再执行动作？**

------



## **💡3. 作者的创新（What They Propose）**

作者提出了 **CoT-VLA**，意思是：

**用视觉链式思维（Visual Chain-of-Thought, 简称 CoT）来增强 VLA 模型的推理能力。**

### **什么是“视觉链式思维”？**

- 就像人类在做事情前，会脑补接下来的画面，比如“先把杯子拿起来，再移动到桌子上，然后放下”。
- CoT-VLA 会先 **一步步生成未来的中间视觉图像（子目标）**，然后再计划每一步该做什么动作来达成这些目标。

------



## **✨4. 为什么这很重要？**

1. **更强的推理能力**：通过视觉想象中间步骤，模型能处理更复杂的任务。
2. **利用更多数据**：因为中间目标是图像，不需要动作标签，所以可以用大量的视频数据训练。
3. **解释性强**：生成的图像作为中间目标，更容易让人理解它的计划过程。

------



## **🛠️5. 它是怎么实现的？（解决方案的细节我们之后详细讲）**

简单先说下结构：

- 把一个任务分成多个 **子目标图像**（未来状态的视觉想象）
- 然后用一个 **动作预测器** 生成一段段短动作序列，逐步达成这些目标



------



## **📊 CoT-VLA 模型结构**

![Refer to caption](../_pages/assetsx1.png)

这张图展示了整个流程分为两个阶段：

### **🔹Step 1:** 

### **视觉链式思维阶段（Visual Chain-of-Thought Reasoning）**

- 输入：任务指令（如“把杯子放在桌子上”）+ 当前观察图像（图像帧）
- 模型生成多个**未来图像（子目标）**：这些图像表示任务完成过程中的中间状态，比如：
  1. 拿起杯子
  2. 移动到桌子上方
  3. 放下杯子

> 这些图像不是直接去执行，而是作为“思考的中间步骤”，让模型“脑补”这个过程。

### **🔹Step 2:** 

### **动作生成阶段（Action Plan Generation）**

- 然后，模型以当前图像 + 每一个子目标图像为输入
- 逐个预测出达成该子目标所需的一小段动作序列（像轨迹片段）

最后，这些动作片段就组成了整个任务的完整动作序列。

------



### **🎯关键点总结**



| **模块**       | **输入**              | **输出**       | **作用**   |
| -------------- | --------------------- | -------------- | ---------- |
| Vision CoT     | 图像 + 指令           | 子目标图像序列 | 拟人化思考 |
| Action Planner | 当前图像 + 子目标图像 | 动作片段       | 执行动作   |

------



你可以想象模型像这样在操作：

> “我现在要完成这个任务，那我得先拿杯子，再移动，再放下… 每一步会是什么画面？好，现在我一步步来。”



![Refer to caption](../_pages/assetsx2.png)





## 1. 我们现在来深入研究：**CoT-VLA 是如何生成子目标图像（goal image）的？**

------



## **🧠 这部分是模型的「视觉链式思维」模块（Visual Chain-of-Thought Reasoning）**

它的目标是：

> 给定当前观察图像（obs image）和语言指令（text），**一步步生成未来的“子目标图像序列”**。

就像人在执行任务前脑补每一步视觉图景，模型也要想象这些视觉状态。

## **🧩 输入和输出**

| **项目** | **内容**                     |
| -------- | ---------------------------- |
| 输入     | 当前图像 + 指令文本          |
| 输出     | 一系列未来图像（子目标状态） |

这些图像不是随便猜的，而是模型通过学习，从过去完成任务的视频中**学会“视觉地思考”**。

## **🛠️ 用的模型结构是什么？**

他们使用的是一种 **token-based 的图像生成 Transformer**：

### **✅ 核心架构类似 MaskGIT / MAGVIT-2：**

- 图像不是直接生成的像素矩阵，而是转成 **视觉 token**（VQ-VAE 编码器将图像编码成离散 token）
- 模型在 token 空间一步步生成目标图像的 token，然后再用解码器还原出真实图像

### **构成模块：**

1. **图像编码器**：把原始图像压缩成 token 序列
2. **文本编码器**（如 T5）：处理指令文本
3. **Transformer（带因果注意力）**：
   - 先输入当前图像 token 和文本 token
   - 然后**自回归地生成未来图像 token**
4. **图像解码器**：把生成的 token 变回图像

## **🔄 生成多步子目标的策略？**

模型每次可以生成一个“子目标图像”，然后：

- 以这个子目标作为新起点，继续生成下一个子目标
- 直到生成固定长度的图像序列，或模型预测终止 token

这就形成了一条视觉推理链（Visual Chain-of-Thought）。

## **💡 举个例子：**

**任务：Move towel to plate**

视觉链式思维可能这样生成：

1. 当前图像 + 指令 → 子目标图像 1（手接近毛巾）
2. 子目标图像 1 → 子目标图像 2（毛巾被抓住）
3. 子目标图像 2 → 子目标图像 3（毛巾在移动中）
4. 子目标图像 3 → 子目标图像 4（毛巾放到盘子上）

这些中间视觉状态就是模型的“思考过程”。

## **📈 训练目标（loss）**

训练时用的 loss 是图像 token 的分类损失（cross-entropy），目标是让生成的 token 尽量接近 ground truth 的图像 token。

如果是使用视频数据（哪怕没有动作标签），也可以学到合理的子目标序列。

![Refer to caption](../_pages/assetsx3.png)



## 2. 我们现在来看论文中第二个关键组件：**动作生成器（Action Generator）**。



## **🧠 目标回顾**

子目标图像生成出来以后，模型要完成第二步：

> 从「当前图像 + 子目标图像 + 语言指令」中，**生成实现这个目标的一段动作序列**（也叫 action chunk）。

比如说，现在的图像是「手在桌子旁」，目标图像是「手抓住杯子」，那就要预测怎么移动手臂去抓杯子。

## **🔧 输入和输出**

| **名称** | **内容**                                                     |
| -------- | ------------------------------------------------------------ |
| 输入     | 当前图像 + 子目标图像 + 语言文本（嵌入）                     |
| 输出     | 一个动作序列 a₁, a₂, …, aₙ（通常是一段较短的 primitive chunk） |

## **📦 模块结构详解**

### **1. 图像编码部分**

- 当前图像（observation image） 和 子目标图像（goal image）都先通过 **图像编码器** 转换成 token 序列（类似 CNN + Flatten/ViT）
- 它们被拼接在一起作为视觉上下文

### **2. 文本编码部分**

- 语言指令用 T5/CLIP text encoder 编码成一组文本 token 或一个向量

### **3. Transformer 模块（核心）**

- 输入包括：
  - 当前图像 token
  - 子目标图像 token
  - 文本 token
- 使用 **Causal Transformer** 或带全注意力的 Transformer（Full Attention），作为 Sequence-to-Sequence 模型
- 输出一段动作序列（每个时间步是一个动作）

### **4. 动作建模方式：**

- 每个动作可能包括：
  - 位移 Δx, Δy, Δz（手的位置）
  - 角度、夹爪开合等（机械臂的完整动作向量）
- 模型可以使用 **自回归方式** 预测每一步动作（像 GPT 预测 token 一样）

## **🧠 模型如何训练？**

使用标准的行为克隆（Behavior Cloning）方法：

- 给定状态和目标 → 模型学着人类/机器人怎么做
- 损失函数是 MSE（预测动作 vs ground truth 动作）

$\mathcal{L}_{\text{BC}} = \sum_t \| \hat{a}_t - a_t \|^2$

每个子目标都会带来一段训练样本（从当前状态走向子目标的动作序列）。

## **🔄 推理时的流程（运行中）：**

> 对每一个子目标图像：

1. 当前图像 + 子目标图像 + 文本 → 输入动作生成器
2. 得到动作序列 a₁, a₂, …, aₖ
3. 控制机器人执行
4. 得到新的图像，再进入下一轮（闭环控制）





## 效果

![Refer to caption](../_pages/assetsx4.png)