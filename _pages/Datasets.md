---
layout: article
title: Datasets
permalink: /datasets/
---

## Ego4D

超大规模的第一人称视频数据集，3670小时覆盖多种日常场景，并提供丰富标注与任务。但缺乏本地手部姿态标注，且不专注于物体操作。

## EgoDex
目前最大最丰富的第一视角灵巧操作数据集，829 小时的第一人称视角视频，配有同步采集的 3D 手部和手指追踪数据。

## EgoMimic

小型版EgoDex，面向**机器人模仿学习**的操作数据集，4h人类数据直接映射为机器人可执行动作，collects only the wrist positions。



## HUMOTO

高精度的人体-物体交互数据集，覆盖真实完整任务流程，提供清洗后的全身动作与多物体交互。

| **人体动作**         | 通常以 **3D骨架关节序列**（如 SMPL）表示，包括身体、手臂、手指的运动。 |
| -------------------- | ------------------------------------------------------------ |
| **物体信息**         | 被操纵物体的 **三维模型、位置、姿态**（甚至物体的关节状态，如门是否打开）。 |
| **时序数据**         | 一段完整任务（如“泡茶”）从开始到结束的全流程，而非一个个碎片动作。 |
| **视觉数据（可选）** | 视频、RGB图像、多视角相机采集到的原始数据。                  |
| **语义标签（可选）** | 每一帧的人体状态、物体状态、动作名称等。                     |



## Being-M0

Scaling Large Motion Models with Million-Level Human Motions

首个百万级人体动作生成数据集，结合层次文本标注与创新编码方案。

| **动作序列**         | 通常为每帧的**3D关节点位置**或参数化的人体模型（如 SMPL）序列。 |
| -------------------- | ------------------------------------------------------------ |
| **语义描述**         | 每段动作配有 **文本说明**（动作是什么、目的是什么、情绪如何）。 |
| **分层结构（可选）** | 有些数据集中，文本标注有高层次任务 + 低层次动作，如“做饭” → “切菜 + 炒菜”。 |



